# Fitting an MCM model

Now that we have set up our MCMSEM model it is time to fit it to our data. For the purposes of this document we will continue with the model we have generated previously. To recreate it run:
```  
my_model <- MCMmodel(dataset, n_latent=2, constrained_a=FALSE, scale_data=FALSE)
my_model <- MCMedit(my_model, "A", c("a1_4", "a1_5", "a2_2", "a2_3"), 0)
```

## MCMfit

Since everything required to fit our MCM model is already stored in our MCMmodel object, all you need to provide to `MCMfit` is the model and the data.

```
my_result <- MCMfit(my_model, dataset)
```

> **Note**: We could store your data in the MCMmodel object, but that would result in unnecessary copies of potentially large datasets.

This will produce an MCM result object which when printed displays the parameters and their standard errors:
``` 
    a1_1 a1_2 a1_3 a2_4 a2_5 b1_2 b1_3 b1_4  b1_5  b2_1 b2_3 ...
est 0.63 0.53 0.48 0.47 0.49 0.27 0.02 0.20 -0.02 -0.04 0.33 ...
se  0.01 0.02 0.02 0.10 0.11 0.02 0.00 0.01  0.01  0.01 0.01 ...
```
Note that even though it looks like it when printed, the MCM result object is not a dataframe. To obtain the estimate and SE dataframe that is printed use `as.data.frame(my_result)`

## Standard errors

By default `MCMfit` will include the calculation of standard errors, however, this may not be ideal, e.g. when simply testing the optimization of larger models. You can turn off this computation by setting `compute_se` to `FALSE`.

```
my_result <- MCMfit(my_model, dataset, compute_se=FALSE)
```

MCMSEM includes 2 options for calculating standard errors: (1) asymptotic approximation, (2) bootstrap. Asymptotic approximation is the default:

```
my_result <- MCMfit(my_model, dataset, se_type='asymptotic')
```

And for bootstrap there are two options: one-step, or two-step. In the two-step bootstrap chunks of the data are bootstrapped, rather than the entire data for a significant boost in performance, therefore two-step is generally recommended.

```
my_result <- MCMfit(my_model, dataset, se_type='one-step') # "Standard" bootstrap
my_result <- MCMfit(my_model, dataset, se_type='two-step') # Chunked bootstrap
```

The number of boostrap iterations can be adjusted via `bootstrap_iter` (defaults to 200), and the number of chunks in two-step boostrap can be adjusted via `bootstrap_chunks` (defaults to 1000).

```
my_result <- MCMfit(my_model, dataset, se_type='two-step', boostrap_iter=100, bootstrap_chunks=500)
```

## Optimizer & loss
MCMSEM by default uses [RPROP](https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html?highlight=rprop#torch.optim.Rprop) and [LBFGS](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html?highlight=lbfgs#torch.optim.LBFGS) optimizers in sequence. 
RPROP serves to find a good starting point, LBFGS to find the optimal soltuion given RPROPs starting point. By default MCMSEM will run RPROP for 50 iterations and LBFGS for 12, this can be changed via `optim_iters`.

```
my_result <- MCMfit(my_model, dataset, optim_iters=c(100, 25))
```

The first optimizer can be changed by passing its lowercase name to the `optimizer` argument. Note that learning rates (see below) can behave differently across different optimizers, so you will likely want to change that in addition to the optimizer.  

``` 
my_result <- MCMfit(my_model, dataset, optimizer='sgd')
```

> The following optimizers are implemented, but again, all but rprop have been tested: 
> [adadelta](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html),
> [adagrad](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html),
> [adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html),
> [asgd](https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html),
> [rmsprop](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html),
> [rprop](https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html),
> [sgd](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)

By default the learning rate for the first optimizer is 0.02, and the learning rate for LBFGS is 1.0. The learning rate for LBFGS is so high because it should already start close to an optimal solution (i.e. gradients should be low) thanks to RPROP.
The learning rates can be adjusted via the `learning_rate` argument. If your estimates all return NA this might be a good starting point (in which case learning rate should be decreased).

```
my_result <- MCMfit(my_model, dataset, learning_rate=c(0.01, 0.5))
```

By default loss calculation is done by mean squared error (`mse`), alternatively you can use a `smooth_l1` loss by specifying the `loss_type` argument

```
my_result <- MCMfit(my_model, dataset, loss_type='smooth_l1')
```

If you want to keep an eye on the loss as the otpimizer runs you can set `silent` to `FALSE`, which will make `MCMfit` print the loss at every step.
```
my_result <- MCMfit(my_model, dataset, silent=FALSE)
```

## Bounds

Due to our use of optimizers that were developed for machine learning applications we are unable to use hard bounds in these optimizers, therefore the default is to not use bounds at all. If you do want to use bounds you can use our work-around described below by setting `use_bounds` to `TRUE`.

```
my_result <- MCMfit(my_model, dataset, use_bounds=TRUE)
```

> As a work-around we have made the loss scale exponentially as estimates get further out of bounds, to nudge the optimizer toward an in-bound solution without destroying the gradients on which the optimizers heavily rely.
> As a result of this it is still possible that `MCMfit` finds a solution where parameters are slightly out of bounds even when you have set `use_bounds` to `TRUE`. In pseudo-code:
>```
> lbound_check = ( estimates <= lbounds ) # 0 if parameter is in bounds, 1 if parameter is below lower bound
> ubound_check = ( estimates >= ubounds ) # 0 if parameter is in bounds, 1 if parameter is above upper bound
> lbound_dist  = ( estimates - lbounds ) ^ 2  # Distance from  lower bounds
> ubound_dist  = ( estimates - ubounds ) ^ 2  # Distance from  upper bounds
> lbound_dist_sum = sum( sqrt( lbound_dist * lbound_check ) )  # Sum of distances from lower bounds
> ubound_dist_sum = sum( sqrt( ubound_dist * ubound_check ) )  # Sum of distances from upper bounds
> pow = lbound_dist_sum + ubound_dist_sum
> loss = loss * ( 2 ^ pow ) 
>```

## Skewness and kurtosis
By default `MCMfit` will fit the full model including both skewness and kurtosis. You can disable either one of these by setting `use_skewness` or `use_kurtosis` to `FALSE`
```
my_result_no_skew <- MCMfit(my_model, dataset, use_skewness=FALSE)
my_result_no_kurt <- MCMfit(my_model, dataset, use_kurtosis=FALSE)
```

When testing larger (>20 variable) models we highly recommend to set `use_kurtosis` to `FALSE` in the testing phase, as this significantly increases performance. Once you have a model you are satisfied with, set `use_kurtosis` to `TRUE` to obtain your final estimates.

## Fitting on a GPU

In order to fit your MCMSEM model on a GPU you must (1) meet all the requirements and (2) have a CUDA-enabled installation of `torch`, see `1. Installing MCMSEM` for more detail.

To run `MCMfit()` on a GPU first, setup a CUDA device, and pass this to `MCMfit()`. This will cause all underlying matrices to be stored on the GPU instead, in turn running optimization on the GPU, potentially improving performance significantly.

``` 
cuda_device <- torch_device("cuda")
my_result <- MCMfit(my_model, dataset, device=cuda_device)
```

### CUDA out of memory

When running into CUDA Out of Memory errors you can first try to set `low_memory` to `TRUE`. This induces a deep garbage collection between every iteration, so it will significantly impact performance, but it will also save a lot of VRAM potentially making your model possible to run on your GPU.
               
```
my_result <- MCMfit(my_model, dataset, device=cuda_device, low_memory=TRUE)
```

If after setting `low_memory` to `TRUE` you still run into CUDA out of memory errors see `1. Installing MCMSEM` for your options.

### Other arguments with GPU

All other arguments to `MCMfit` work with a CUDA device, so you do not need to change anything about your code when moving from CPU to GPU. 
The only difference being that `silent` is automatically enabled when using a GPU, since the loss is stored on the GPU itself, moving it from VRAM to RAM and then to the console would significantly impact performance.